{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(Pkg.PackageSpec(url=\"https://github.com/JuliaComputing/JuliaAcademyData.jl\"))\n",
    "using JuliaAcademyData; activate(\"Parallel_Computing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed (or multi-core or multi-process) parallelism\n",
    "\n",
    "Julia has a built-in standard library — Distributed — that allows you to\n",
    "start and run multiple concurrent Julia processes. Imagine starting a slew\n",
    "of Julia instances and then having an easy way to run code on each and every\n",
    "one of them; that's what Distributed provides.\n",
    "\n",
    "![](https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Parallel_Computing/images/Julia6x.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(4)\n",
    "@sync @everywhere workers() include(\"/opt/julia-1.0/etc/julia/startup.jl\") # Needed just for JuliaBox\n",
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily communicate with the other nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = @spawnat 2 (myid(), rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works kinda like an `@async` task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time r = @spawnat 2 (sleep(1), rand())\n",
    "@time fetch(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can repeat the same examples from tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time for i in 2:nprocs() # proc 1 is the controlling node\n",
    "    @spawnat i sleep(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time @sync for i in 2:nprocs()\n",
    "    @spawnat i sleep(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except unlike tasks, we're executing the code on a separate process — which\n",
    "can be performed on a different processor in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function work(N)\n",
    "    series = 1.0\n",
    "    for i in 1:N\n",
    "        series += (isodd(i) ? -1 : 1) / (i*2+1)\n",
    "    end\n",
    "    return 4*series\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time work(1_000_000_000)\n",
    "@time @sync for i in workers()\n",
    "    @spawnat i work(1_000_000_000)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this isn't very helpful. We're just performing exactly the same\n",
    "calculation on every worker... and then completely ignoring the result! Let's\n",
    "restructure our computation to be a bit more parallel friendly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function partial_pi(r)\n",
    "    series = 0.0\n",
    "    for i in r\n",
    "        series += (isodd(i) ? -1 : 1) / (i*2+1)\n",
    "    end\n",
    "    return 4*series\n",
    "end\n",
    "a = partial_pi(0:999)\n",
    "a, a-pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = partial_pi(1000:9999)\n",
    "(a + b), (a+b) - pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can distribute this computation across our many workers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0:10_000_000_000\n",
    "futures = Array{Future}(undef, nworkers())\n",
    "@time begin\n",
    "    for (i, id) in enumerate(workers())\n",
    "        batch = 0:length(r)÷nworkers()-1\n",
    "        futures[i] = @spawnat id partial_pi(batch .+ (i-1)*(length(r)÷nworkers()))\n",
    "    end\n",
    "    p = sum(fetch.(futures))\n",
    "end\n",
    "p - pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,0. + π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's rather annoying — needing to carefully divide up our workflow and\n",
    "manually collect all our results and such.  There's an easier way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time p = @distributed (+) for r in [(0:9999) .+ offset for offset in 0:10000:r[end]-1]\n",
    "    partial_pi(r)\n",
    "end\n",
    "p - pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this different from `@threads for` and `@simd for`? Why not just\n",
    "`@distributed for`?  Why the `@distributed (+) for`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: Moving data is _expensive_!\n",
    "\n",
    "| System Event                   | Actual Latency | Scaled Latency |\n",
    "| ------------------------------ | -------------- | -------------- |\n",
    "| One CPU cycle                  |     0.4 ns     |     1 s        |\n",
    "| Level 1 cache access           |     0.9 ns     |     2 s        |\n",
    "| Level 2 cache access           |     2.8 ns     |     7 s        |\n",
    "| Level 3 cache access           |      28 ns     |     1 min      |\n",
    "| Main memory access (DDR DIMM)  |    ~100 ns     |     4 min      |\n",
    "| Intel Optane memory access     |     <10 μs     |     7 hrs      |\n",
    "| NVMe SSD I/O                   |     ~25 μs     |    17 hrs      |\n",
    "| SSD I/O                        |  50–150 μs     | 1.5–4 days     |\n",
    "| Rotational disk I/O            |    1–10 ms     |   1–9 months   |\n",
    "| Internet call: SF to NYC       |      65 ms     |     5 years    |\n",
    "| Internet call: SF to Hong Kong |     141 ms     |    11 years    |\n",
    "\n",
    "You really don't want to be taking a trip to the moon very frequently.\n",
    "Communication between processes can indeed be as expensive as hitting a disk —\n",
    "sometimes they're even implemented that way.\n",
    "\n",
    "So that's why Julia has special support for reductions built in to the\n",
    "`@distributed` macro: each worker can do its own (intermediate) reduction\n",
    "before returning just one value to our master node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes you need to see those intermediate values. If you have a\n",
    "very expensive computation relative to the communication overhead, there are\n",
    "several ways to do this. The easiest is `pmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time pmap(partial_pi, [(0:99999) .+ offset for offset in 0:100000:r[end]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we have a large computation relative to the number of return values,\n",
    "pmap is great and easy.\n",
    "\n",
    "Increase the work on each worker by 100x and reduce the amount of communication by 100x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time pmap(partial_pi, [(0:9999999) .+ offset for offset in 0:10000000:r[end]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways of doing this, though, too — we'll get to them in a minute.\n",
    "But first, there's something else that I glossed over: the `@everywhere`s above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node is _completely_ independent; it's like starting brand new, separate\n",
    "Julia processes yourself. By default, `addprocs()` just launches the\n",
    "appropriate number of workers for the current workstation that you're on, but\n",
    "you can easily connect them to remote machines via SSH or even through cluster\n",
    "managers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those `@everywhere`s above are very important! They run the given expression\n",
    "on all workers to make sure the state between them is consistent. Without it,\n",
    "you'll see errors like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello() = \"hello world\"\n",
    "r = @spawnat 2 hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this applies to packages, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics # The Statistics stdlib defines mean\n",
    "fetch(@spawnat 2 mean(rand(100_000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using Statistics\n",
    "fetch(@spawnat 2 mean(rand(100_000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ways to structure and/or share data between processes\n",
    "\n",
    "Unlike `@threads`, we no longer have access to the same memory. While this\n",
    "does make expressing some algorithms a little more tricky, the \"default\"\n",
    "is much safer! There isn't any shared state to begin with, so it's harder\n",
    "to write an incorrect algorithm. It's also just harder to write some\n",
    "algorithms in the first place.\n",
    "\n",
    "So there are some special array types that can help bridge the gap between\n",
    "processes and make writing parallel code a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `SharedArray`\n",
    "\n",
    "If all workers are on the same physical machine, while they cannot share\n",
    "memory, they do all have shared access to the same hard drive(s)!\n",
    "\n",
    "The `SharedArray` makes use of this fact, allowing concurrent accesses to the\n",
    "same array — somewhat akin to threads default state.\n",
    "\n",
    "This is the prefix definition from the \"thinking in parallel\" course:\n",
    "\n",
    "```\n",
    "using .Threads\n",
    "function prefix_threads!(y, ⊕)\n",
    "    l=length(y)\n",
    "    k=ceil(Int, log2(l))\n",
    "    for j=1:k\n",
    "        @threads for i=2^j:2^j:min(l, 2^k)       #\"reduce\"\n",
    "            @inbounds y[i] = y[i-2^(j-1)] ⊕ y[i]\n",
    "        end\n",
    "    end\n",
    "    for j=(k-1):-1:1\n",
    "        @threads for i=3*2^(j-1):2^j:min(l, 2^k) #\"expand\"\n",
    "            @inbounds y[i] = y[i-2^(j-1)] ⊕ y[i]\n",
    "        end\n",
    "    end\n",
    "    y\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SharedArrays\n",
    "function prefix!(y::SharedArray, ⊕)\n",
    "    l=length(y)\n",
    "    k=ceil(Int, log2(l))\n",
    "    for j=1:k\n",
    "        @distributed for i=2^j:2^j:min(l, 2^k)       #\"reduce\"\n",
    "            @inbounds y[i] = y[i-2^(j-1)] ⊕ y[i]\n",
    "        end\n",
    "    end\n",
    "    for j=(k-1):-1:1\n",
    "        @distributed for i=3*2^(j-1):2^j:min(l, 2^k) #\"expand\"\n",
    "            @inbounds y[i] = y[i-2^(j-1)] ⊕ y[i]\n",
    "        end\n",
    "    end\n",
    "    y\n",
    "end\n",
    "data = rand(1_000_000);\n",
    "A = SharedArray(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix!(SharedArray(data), +) # compile\n",
    "@time prefix!(A, +);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A ≈ cumsum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What went wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function prefix!(y::SharedArray, ⊕)\n",
    "    l=length(y)\n",
    "    k=ceil(Int, log2(l))\n",
    "    for j=1:k\n",
    "        @sync @distributed for i=2^j:2^j:min(l, 2^k)       #\"reduce\"\n",
    "            @inbounds y[i] = y[i-2^(j-1)] ⊕ y[i]\n",
    "        end\n",
    "    end\n",
    "    for j=(k-1):-1:1\n",
    "        @sync @distributed for i=3*2^(j-1):2^j:min(l, 2^k) #\"expand\"\n",
    "            @inbounds y[i] = y[i-2^(j-1)] ⊕ y[i]\n",
    "        end\n",
    "    end\n",
    "    y\n",
    "end\n",
    "A = SharedArray(data)\n",
    "@time prefix!(A, +)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A ≈ cumsum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistributedArrays\n",
    "\n",
    "We can, though, turn the problem on its head and allow the _data_ itself\n",
    "to determine how the problem gets split up. This can save us tons of indexing\n",
    "headaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using Distributed\n",
    "@everywhere using DistributedArrays\n",
    "A = DArray(I->fill(myid(), length.(I)), (24, 24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument takes a function that transforms the given set of indices\n",
    "to the _local portion_ of the distributed array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = DArray((24,24)) do I\n",
    "    @show I\n",
    "    fill(myid(), length.(I))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that none of the array actually lives on processor 1, but we can still\n",
    "display the contents — when we do we're requesting all workers give us their\n",
    "current data! While we've only talked about master-worker communcation so far,\n",
    "workers can communicate directly amongst themselves, too (by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using BenchmarkTools\n",
    "fetch(@spawnat 2 @benchmark $A[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(@spawnat 2 @benchmark $A[end,end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's fastest to work on a `DArray`'s \"local\" portion, but it's _possible_\n",
    "to grab other data if need be. This is perfect for any sort of tiled operation\n",
    "that works on neighboring values (like image filtering/convolution). Or Conway's\n",
    "game of life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function life_step(d::DArray)\n",
    "    DArray(size(d),procs(d)) do I\n",
    "        # Compute the indices of the outside edge (that will come from other processors)\n",
    "        top   = mod1(first(I[1])-1,size(d,1))\n",
    "        bot   = mod1( last(I[1])+1,size(d,1))\n",
    "        left  = mod1(first(I[2])-1,size(d,2))\n",
    "        right = mod1( last(I[2])+1,size(d,2))\n",
    "        # Create a new, temporary array that holds the local part + outside edge\n",
    "        old = Array{Bool}(undef, length(I[1])+2, length(I[2])+2)\n",
    "        # These accesses will pull data from other processors\n",
    "        old[1      , 1      ] = d[top , left]\n",
    "        old[2:end-1, 1      ] = d[I[1], left]   # left side (and corners)\n",
    "        old[end    , 1      ] = d[bot , left]\n",
    "        old[1      , end    ] = d[top , right]\n",
    "        old[2:end-1, end    ] = d[I[1], right]  # right side (and corners)\n",
    "        old[end    , end    ] = d[bot , right]\n",
    "        old[1      , 2:end-1] = d[top , I[2]]   # top\n",
    "        old[end    , 2:end-1] = d[bot , I[2]]   # bottom\n",
    "        # But this big one is all local!\n",
    "        old[2:end-1, 2:end-1] = d[I[1], I[2]]   # middle\n",
    "\n",
    "        life_rule(old) # Compute the new segment!\n",
    "    end\n",
    "end\n",
    "@everywhere function life_rule(old)\n",
    "    # Now this part — the computational part — is entirely local and on Arrays!\n",
    "    m, n = size(old)\n",
    "    new = similar(old, m-2, n-2)\n",
    "    for j = 2:n-1\n",
    "        @inbounds for i = 2:m-1\n",
    "            nc = (+)(old[i-1,j-1], old[i-1,j], old[i-1,j+1],\n",
    "                     old[i  ,j-1],             old[i  ,j+1],\n",
    "                     old[i+1,j-1], old[i+1,j], old[i+1,j+1])\n",
    "            new[i-1,j-1] = (nc == 3 || nc == 2 && old[i,j])\n",
    "        end\n",
    "    end\n",
    "    new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = DArray(I->rand(Bool, length.(I)), (20,20))\n",
    "using Colors\n",
    "Gray.(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = copy(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = Gray.(life_step(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters and more ways to distribute\n",
    "\n",
    "You can easily connect to completely separate machines with SSH access built in!\n",
    "But there are many other ways to connect to clusters:\n",
    "\n",
    "* [JuliaRun](https://juliacomputing.com/products/juliarun)\n",
    "* [Kubernetes](https://juliacomputing.com/blog/2018/12/15/kuber.html)\n",
    "* [MPI](https://github.com/JuliaParallel/MPI.jl)\n",
    "* [Cluster job queues with ClusterManagers](https://github.com/JuliaParallel/ClusterManagers.jl)\n",
    "* [Hadoop](https://github.com/JuliaParallel/Elly.jl)\n",
    "* [Spark](https://github.com/dfdx/Spark.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-process parallelism is the heavy-duty workhorse in Julia\n",
    "\n",
    "It can tackle very large problems and distribute across a very large number\n",
    "of workers. Key things to remember\n",
    "\n",
    "* Each worker is a completely independent Julia process\n",
    "    * Data must move to them\n",
    "    * Code must move to them\n",
    "* Structure your algorithms and use a distributed mechanism that fits with the\n",
    "  time and memory parameters of your problem\n",
    "    * `@distributed` can be good for reductions and even relatively fast inner loops with limited (or no) explicit data transfer\n",
    "    * `pmap` is great for very expensive inner loops that return a value\n",
    "    * `SharedArray`s can be an easier drop-in replacement for threading-like behaviors (on a single machine)\n",
    "    * `DistributedArray`s can turn the problem on its head and let the data do the work splitting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
